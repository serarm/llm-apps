{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEKghJQ2pmYH"
      },
      "source": [
        "### The Basics of LangChain\n",
        "\n",
        "In this notebook we'll explore exactly what LangChain is doing - and implement a straightforward example that lets us ask questions of a document!\n",
        "\n",
        "First things first, let's get our dependencies all set!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fXsYHTgvnCM2"
      },
      "outputs": [],
      "source": [
        "!pip install openai langchain -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0sLjfy8p3jf"
      },
      "source": [
        "You'll need to have an OpenAI API key for this next part - see [this](https://www.onmsft.com/how-to/how-to-get-an-openai-api-key/) if you haven't already set one up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TTosnCHnGHG",
        "outputId": "f2329f6d-c440-4f7d-f41f-09a1d63308fe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "openai_api_key = getpass.getpass(\"OpenAI API Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15M3Jx6SBXcO"
      },
      "source": [
        "#### Helper Functions (run this cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k3SBzWBUpQ21"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def disp_markdown(text: str) -> None:\n",
        "  display(Markdown(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fU4LWrv-BayH"
      },
      "source": [
        "### Our First LangChain ChatModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-M-VQhQOC1c"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "<div class=\"warn\">Note: Information on OpenAI's <a href=https://openai.com/pricing>pricing</a> and <a href=https://openai.com/policies/usage-policies>usage policies.</a></div>\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVkfqk4NOFWS"
      },
      "source": [
        "Now that we're set-up with OpenAI's API - we can begin making our first ChatModel!\n",
        "\n",
        "There's a few important things to consider when we're using LangChain's ChatModel that are outlined [here](https://python.langchain.com/en/latest/modules/models/chat.html)\n",
        "\n",
        "Let's begin by initializing the model with OpenAI's `gpt-3.5-turbo` (ChatGPT) model.\n",
        "\n",
        "We're not going to be leveraging the [streaming](https://python.langchain.com/en/latest/modules/models/chat/examples/streaming.html) capabilities in this Notebook - just the basics to get us started!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tNscLft_nxBb"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.schema import HumanMessage\n",
        "\n",
        "chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzGhlpwUPyU9"
      },
      "source": [
        "If we look at the [Chat completions](https://platform.openai.com/docs/guides/chat) documentation for OpenAI's chat models - we'll see that there are a few specific fields we'll need to concern ourselves with:\n",
        "\n",
        "`role`\n",
        "- This refers to one of three \"roles\" that interact with the model in specific ways.\n",
        "- The `system` role is an optional role that can be used to guide the model toward a specific task. Examples of `system` messages might be:\n",
        "  - You are an expert in Python, please answer questions as though we were in a peer coding session.\n",
        "  - You are the world's leading expert in stamps.\n",
        "\n",
        "  These messages help us \"prime\" the model to be more aligned with our desired task!\n",
        "\n",
        "- The `user` role represents, well, the user!\n",
        "- The `assistant` role lets us act in the place of the model's outputs. We can (and will) leverage this for some few-shot prompt engineering!\n",
        "\n",
        "Each of these roles has a class in LangChain to make it nice and easy for us to use!\n",
        "\n",
        "Let's look at an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dM7lciZtoPEp"
      },
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")\n",
        "\n",
        "# The SystemMessage is associated with the system role\n",
        "system_message = SystemMessage(content=\"You are a food critic.\")\n",
        "\n",
        "# The HumanMessage is associated with the user role\n",
        "user_message = HumanMessage(content=\"Do you think Kraft Dinner constitues fine dining?\")\n",
        "\n",
        "# The AIMessage is associated with the assistant role\n",
        "assistant_message = AIMessage(content=\"Egads! No, it most certainly does not!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSx5HBgjSUvB"
      },
      "source": [
        "Now that we have those messages set-up, let's send them to `gpt-3.5-turbo` with a new user message and see how it does!\n",
        "\n",
        "It's easy enough to do this - the ChatOpenAI model accepts a list of inputs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwDLOYOKSTpG",
        "outputId": "c3fc612a-cc0c-43a3-daed-40dac21d9172"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Well, while Red Lobster does offer a casual dining experience with a focus on seafood, it is not typically considered fine dining. Fine dining establishments are known for their exquisite cuisine, attention to detail, elegant atmosphere, and impeccable service. Red Lobster, on the other hand, is more of a family-friendly, casual seafood chain restaurant. While they may serve tasty dishes, it falls more into the category of casual dining rather than fine dining.')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "second_user_message = HumanMessage(content=\"What about Red Lobster, surely that is fine dining!\")\n",
        "\n",
        "# create the list of prompts\n",
        "list_of_prompts = [\n",
        "    system_message,\n",
        "    user_message,\n",
        "    assistant_message,\n",
        "    second_user_message\n",
        "]\n",
        "\n",
        "# we can just call our chat_model on the list of prompts!\n",
        "chat_model(list_of_prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZMYJDWXTkMq"
      },
      "source": [
        "Great! That's inline with what we expected to see!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DUNhabQUB8f"
      },
      "source": [
        "### PromptTemplates\n",
        "\n",
        "Next stop, we'll discuss a few templates. This allows us to easily interact with our model by not having to redo work we've already completed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "74vpojywT0-4"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")\n",
        "\n",
        "# we can signify variables we want access to by wrapping them in {}\n",
        "system_prompt_template = \"You are an expert in {SUBJECT}, and you're currently feeling {MOOD}\"\n",
        "system_prompt_template = SystemMessagePromptTemplate.from_template(system_prompt_template)\n",
        "\n",
        "user_prompt_template = \"{CONTENT}\"\n",
        "user_prompt_template = HumanMessagePromptTemplate.from_template(user_prompt_template)\n",
        "\n",
        "# put them together into a ChatPromptTemplate\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_prompt_template, user_prompt_template])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-nbEW-kV_na"
      },
      "source": [
        "Now that we have our `chat_prompt` set-up with the templates - let's see how we can easily format them with our content!\n",
        "\n",
        "NOTE: `disp_markdown` is just a helper function to display the formatted markdown response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "P4vd-W2FV7Xq",
        "outputId": "126bd4ba-4805-48ea-cb37-091d6c1ae2cb"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! As an expert in cheeses, I can tell you that there are many exceptional cheeses from all around the world. However, I must apologize as I'm currently feeling quite tired. Is there anything specific you would like to know about cheeses or any particular type of cheese you're interested in?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# note the method `to_messages()`, that's what converts our formatted prompt into\n",
        "formatted_chat_prompt = chat_prompt.format_prompt(SUBJECT=\"cheeses\", MOOD=\"quite tired\", CONTENT=\"Hi, what are the finest cheeses?\").to_messages()\n",
        "\n",
        "disp_markdown(chat_model(formatted_chat_prompt).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHehNFjAXbU_"
      },
      "source": [
        "### Putting the Chain in LangChain\n",
        "\n",
        "In essense, a chain is exactly as it sounds - it helps us chain actions together.\n",
        "\n",
        "Let's take a look at an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "id": "lTzw4ZMoWX0X",
        "outputId": "ac8249c5-3d05-4c8e-9096-dd79f1811b9e"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "No, it's not a good vehicle. It's a fantastic vehicle! The 1967 Chevrolet Impala is an iconic classic car that is highly sought after by car enthusiasts and collectors. It has a sleek and stylish design, powerful V8 engines, and a smooth ride. It's a true symbol of American automotive history and a dream car for many. So yes, the '67 Impala is an exceptional vehicle!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=chat_model, prompt=chat_prompt)\n",
        "\n",
        "disp_markdown(chain.run(SUBJECT=\"classic cars\", MOOD=\"angry\", CONTENT=\"Is the 67 Chevrolet Impala a good vehicle?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md5XYaAj_t51"
      },
      "source": [
        "### Index Local Files\n",
        "\n",
        "Now that we've got our first chain running, let's talk about indexing and what we can do with it!\n",
        "\n",
        "For the purposes of this tutorial, we'll be using the word \"index\" to refer to a collection of documents organized in a way that is easy for LangChain to access them as a \"Retriever\".\n",
        "\n",
        "Let's check out the Retriever set-up! First, a new dependency!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7mkmPs3GAQMp"
      },
      "outputs": [],
      "source": [
        "!pip install chromadb tiktoken nltk -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dNp7hVQGLFn",
        "outputId": "1dcdf36d-1f5e-47a0-dd22-e2a2f632841e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /Users/serjesh/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFe3eeJTB37W"
      },
      "source": [
        "Before we can get started with our chain - we'll have to include some kind of text that we want to include as potential context.\n",
        "\n",
        "Let's use Douglas Adam's [The Hitch Hiker's Guide to the Galaxy](https://erki.lap.ee/failid/raamatud/guide1.txt) as our text file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vWU68TL2Acpe",
        "outputId": "35815658-dd80-4dbd-eb78-107c04b2bb2e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/serjesh/Documents/tech/llm_course/ai_maker_space/llm-ops-cohort2/git/llm-apps/ai-spacemaker/llm-cohort2/notebooks'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdkQUoQICVwa",
        "outputId": "9dfcd427-7773-4b82-e30c-9e9209bd0ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-03 12:27:56--  https://erki.lap.ee/failid/raamatud/guide1.txt\n",
            "Resolving erki.lap.ee (erki.lap.ee)... 185.158.177.102\n",
            "Connecting to erki.lap.ee (erki.lap.ee)|185.158.177.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 291862 (285K) [text/plain]\n",
            "Saving to: ‘guide1.txt’\n",
            "\n",
            "guide1.txt          100%[===================>] 285.02K   516KB/s    in 0.6s    \n",
            "\n",
            "2023-10-03 12:27:58 (516 KB/s) - ‘guide1.txt’ saved [291862/291862]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://erki.lap.ee/failid/raamatud/guide1.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "!mv guide1.txt ../data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "W7zuITDYCaXo"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "loader = TextLoader('../data/guide1.txt', encoding='utf8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xha9YA4wA1-b"
      },
      "source": [
        "Now we can set up our first Index!\n",
        "\n",
        "More detail can be found [here](https://python.langchain.com/en/latest/modules/indexes/getting_started.html) but we'll skip to a more functional implementation!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JuuKSPgTB0Uz"
      },
      "outputs": [],
      "source": [
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "index = VectorstoreIndexCreator().from_loaders([loader])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nQNNB8NC-XP"
      },
      "source": [
        "Now that we have our Index set-up, we can query it straight away!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7vSgpomC9n9",
        "outputId": "3a52155d-8fed-4028-9571-2240f3291244"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'What are the importances of towels?',\n",
              " 'answer': ' A towel is a massively useful item for an interstellar hitch hiker, as it can be used for warmth, protection, and signaling for help. It also has immense psychological value, as it can make a strag (non-hitch hiker) assume that the hitch hiker is well-prepared and capable.\\n',\n",
              " 'sources': 'guide1.txt'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"What are the importances of towels?\"\n",
        "index.query_with_sources(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F5tv4ULERDo"
      },
      "source": [
        "### Putting it All Together\n",
        "\n",
        "Now that we have a simple idea of how we prompt, what a chain is, and has some local data - let's put it all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "uAHJHsksEx5H"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import NLTKTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.indexes.vectorstore import VectorstoreIndexCreator\n",
        "\n",
        "with open(\"../data/guide1.txt\") as f:\n",
        "    hitchhikersguide = f.read()\n",
        "\n",
        "text_splitter = NLTKTextSplitter()\n",
        "texts = text_splitter.split_text(hitchhikersguide)\n",
        "\n",
        "embeddings = OpenAIEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4Z796M-aE_lU"
      },
      "outputs": [],
      "source": [
        "docsearch = Chroma.from_texts(texts, embeddings, metadatas=[{\"source\": str(i)} for i in range(len(texts))]).as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsEM-dR1FEvp",
        "outputId": "0016e3ab-4c63-48ba-da42-2a858013c908"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-Cdg8XtIG3WjRFquQiNg6yWVc on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-Cdg8XtIG3WjRFquQiNg6yWVc on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-Cdg8XtIG3WjRFquQiNg6yWVc on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-Cdg8XtIG3WjRFquQiNg6yWVc on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'output_text': '\\n\\nThe wind speed velocity of a swallow is not mentioned in the context information.'}"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"refine\")\n",
        "query = \"What is the wind speed velocity of a swallow?\"\n",
        "docs = docsearch.get_relevant_documents(query)\n",
        "chain({\"input_documents\": docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBhx_8q20dq7"
      },
      "source": [
        "This notebook was authored by [Chris Alexiuk](https://www.linkedin.com/in/csalexiuk/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
