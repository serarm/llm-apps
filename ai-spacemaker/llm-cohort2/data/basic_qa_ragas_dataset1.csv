question,answer,contexts,ground_truths
What are the advantages of retrieval-augmented text generation compared to generation-based models?,"Retrieval-augmented text generation has remarkable advantages compared to generation-based models. It has achieved state-of-the-art performance in many NLP tasks. Additionally, retrieval-augmented text generation can bridge the gap between structured data and natural language text, making it suitable for data-to-text generation tasks.","['lemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented'
 'lemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented'
 'and dense memories) and obtained an improved\nmodel performance.\nData-to-Text Generation\nRecently, retrieval-\naugmented generation has been adapted to the task\nof data-to-text generation. To bridge the gap be-\ntween the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrieval-\naugmented framework.\nSpeciﬁcally, given the\nsource data, a set of candidate texts are ﬁrst re-\ntrieved from a large unlabelled corpus. Then, a']","['Retrieval-augmented text generation has several advantages over conventional generation models. Firstly, the knowledge is not necessarily stored implicitly in model parameters, but is explicitly acquired in a plug-and-play manner, which leads to great scalability. Secondly, instead of generating from scratch, this paradigm generates text from some retrieved human-written reference, which potentially alleviates the difficulty of text generation. This approach has achieved state-of-the-art performance in many NLP tasks.']"
What are the three major components of the retrieval-augmented generation paradigm?,"The three major components of the retrieval-augmented generation paradigm are:

1. Retrieval Sources: These are the sources of information that are used for retrieval in the generation process. They can include training corpora, external data, and unsupervised data.

2. Retrieval Metrics: These metrics are used to measure the relevance or similarity between the retrieved information and the generation task. They can include sparse-vector retrieval, dense-vector retrieval, and task-specific retrieval.

3. Generation Model: This is the model that generates the final output based on the retrieved information. It takes into account the retrieval results and uses them to enhance the generation process.","['∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 '∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 '2\nRetrieval-Augmented Paradigm\nIn this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine']","['The three major components of the retrieval-augmented generation paradigm are the retrieval source, retrieval metric, and integration methods.']"
What is the main idea behind the cross-lingual retriever in the context of unsupervised corpus?,"The main idea behind the cross-lingual retriever in the context of unsupervised corpus is to align source-side sentences with their corresponding target-side translations in a dense vector space. This alignment allows the retriever to directly connect the source-side input with the target-side translations, enabling the retrieval of target sentences from an unsupervised corpus in the target language.","['pairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-'
 'pairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-'
 'Since dense retrieval has the potential of cross-\nlingual retrieval, Zheng et al. (2021b) use a similar\napproach to achieve unsupervised domain adapta-\ntion, where a main change is to create the datastore\nbased on synthetic sources sentence and the real\ntarget sentences.\nTraining Phase\nDifferent from those model-\nagnostic approaches, previous works in this line\naim to train the generation model to learn how\nto cooperate with the retrieval model. It is also']","['The main idea behind the cross-lingual retriever in the context of an unsupervised corpus is to align source-side sentences and their corresponding target-side translations in a dense vector space. This is done even when the source-side reference (xr) is absent. As a result, the retriever directly connects the source-side input and target-side translations, enabling monolingual data in the target language to be used alone as memories.']"
What are the two categories of dialogue systems mentioned in the context?,The two categories of dialogue systems mentioned in the context are chit-chat systems and task-oriented systems.,"['3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity'
 '3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity'
 'Most modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\ntions and possibly post-edited by a human. How-']",['The two categories of dialogue systems mentioned in the context are chit-chat systems and task-oriented systems.']
What are the three major limitations in existing work for dialogue response generation?,"The three major limitations in existing work for dialogue response generation are:

1. Current methods only use one retrieved response for generation. It can be more beneficial to combine multiple retrieval responses, but this is difficult due to the one-to-many nature of dialogue response generation.

2. Current methods use a universal relevance score for retrieval. It can be more effective if we can explore different forms of knowledge, such as knowledge bases and external documents, to improve the relevance of the generated responses.

3. The existing work has not fully explored knowledge-grounded dialogue response generation. While there have been some efforts to incorporate knowledge from various sources, there is still room for further exploration and improvement in this area.","['actively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can'
 'actively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can'
 'knowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one']","['The three major limitations in existing work for dialogue response generation are: 1) Current methods only use one retrieved response for generation, when it could be more beneficial to combine multiple retrieval responses. However, this can be difficult due to the one-to-many nature of dialogue response generation. 2) Current methods use a universal relevance score for retrieval. It could be more effective if a more customized retrieval metric was used, especially for controlled dialogue response generation (e.g., persona, emotion, etc). 3) The retrieval pool of existing methods is limited to dialogue corpora (context-response pairs) or documents. It might be useful to enlarge the retrieval pool by including more corpora in other domains or in other modalities. These limitations suggest several possible directions for future exploration.']"
What are the limitations of the three kinds of methods discussed in the context?,"The limitations of the three kinds of methods discussed in the context are:

1. They rely on fuzzy scores for retrieval, which heavily depend on word matching and cannot recall examples that are similar in word semantics but different in surface form.

2. The integration of retrieved examples into the SMT module does not fully utilize the knowledge in the retrieved examples.

3. The current methods only use one retrieved response for generation, while it could be more beneficial to combine multiple retrieval responses. Additionally, the use of a universal relevance score for retrieval could be more effective if it can be customized.","['SMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the ﬁrst two'
 'SMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the ﬁrst two'
 'actively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can']","['The limitations of the three methods discussed in the context are: 1) They all employ a fuzzy score for retrieval which is highly dependent on word matching and thus cannot recall examples that are similar in word semantics but different in surface form. 2) These methods integrate the retrieved examples into a module of SMT in ways that cannot make full use of the knowledge in retrieved examples. For instance, the integration methods in the first two kinds (constrained decoding and phrase table aggregation) are heuristic and not optimized towards translation quality; the parameter tuning method fine-tunes few parameters for log-linear based SMT which are not enough to preserve sufficient knowledge from retrieved examples. 3) Since SMT performs in a pipeline manner, it is intractable to jointly optimize retrieval metrics as well as SMT models. Consequently, all these methods adopt an off-the-shelf metric for retrieval, leading to sub-optimal results.']"
What is the key point of literature in the inference phase of Translation Memory in NMT?,The key point of literature in the inference phase of Translation Memory in NMT is to reward some target words based on words in the retrieval process. This allows for a decision to be made based on both the distribution of the generation model and the additional reward of the retrieval model.,"['previous works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to'
 'previous works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to'
 'primary feature to derive reward scores.\nHow-\never, some information, e.g., frequencies of words\nand context, may also be beneﬁcial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the re-\ntrieved information and when not. In the inference\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation efﬁciency but may\nalso dampen the ﬂuency of generated results.\n5']","['The key point of literature in the inference phase of Translation Memory in NMT is to reward some target words based on words in yr in the inference process. This allows a decision to be made based on both the distribution of the generation model and the additional reward of the retrieval model. Some previous works propose to reward target words based on the sentence-level similarity between x and xr, and the word alignment between xr and yr. Other works reward target words based on token-level similarity score. Some methods also use a light-weight network to learn the reward score.']"
What is the proposed method by Cai et al. (2021) for retrieving the most similar target sentence in a monolingual dataset?,"The proposed method by Cai et al. (2021) is a cross-lingual retriever that directly retrieves the most similar target sentence from an unsupervised corpus in the target language. The main idea is to align source-side sentences and their corresponding target-side translations in a dense vector space. This alignment allows the retriever to connect the source-side input with the target-side translations, even when the source-side translation is absent.","['pairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-'
 'pairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-'
 '2019; Cao et al., 2019). Xia et al. (2019) repre-\nsent the retrieved target sentences in a different\ndata structure, i.e., a graph structure, and integrate\nit through attention mechanism. He et al. (2021)\npropose a light-weight method to encode the re-\ntrieved target sentences and leverage the alignment\ninformation to ﬁlter out irrelevant information. Dif-\nferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework']",['Cai et al. (2021) propose a framework that retrieves the most similar target sentence in a monolingual dataset using a source sentence as a query.']
What is the proposed framework for paraphrase generation by Kazemnejad et al. (2020)?,"The proposed framework for paraphrase generation by Kazemnejad et al. (2020) involves a two-step process. First, a sentence that is similar to the input sentence is retrieved. Then, a neural editor uses the retrieved sentence to produce the resulting paraphrased sentence.","['the selected templates.\nParaphrase Generation\nTo address the lack of\nquality as well as diversity in the generation of para-\nphrases, Kazemnejad et al. (2020) propose a gen-\neration framework which ﬁrst retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in'
 'the selected templates.\nParaphrase Generation\nTo address the lack of\nquality as well as diversity in the generation of para-\nphrases, Kazemnejad et al. (2020) propose a gen-\neration framework which ﬁrst retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in'
 'Amirhossein Kazemnejad, Mohammadreza Salehi, and\nMahdieh Soleymani Baghshah. 2020.\nParaphrase\ngeneration by learning how to edit from samples. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 6010–\n6021, Online. Association for Computational Lin-\nguistics.\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke\nZettlemoyer,\nand Mike Lewis. 2020a.\nNear-\nest neighbor machine translation.\narXiv preprint\narXiv:2010.00710.']","['Kazemnejad et al. (2020) proposed a paraphrase generation framework that first retrieves a sentence similar to the input sentence. Then, a neural editor produces the resulting paraphrased sentence based on the retrieved sentence.']"
What is the purpose of this paper?,"The purpose of this paper is to conduct a survey on retrieval-augmented text generation in the field of computational linguistics. It aims to highlight the generic paradigm of retrieval-augmented generation and review notable approaches in different tasks such as dialogue response generation, machine translation, and other generation tasks. Additionally, the paper points out promising directions for future research in this area.","['the-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction'
 'the-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction'
 'lemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented']","['The purpose of this paper is to conduct a survey about retrieval-augmented text generation. It highlights the generic paradigm of retrieval-augmented generation and reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. The paper also points out some important directions on top of recent methods to facilitate future research.']"
