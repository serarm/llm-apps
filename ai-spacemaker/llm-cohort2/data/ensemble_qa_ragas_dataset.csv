question,answer,contexts,ground_truths
What are the advantages of retrieval-augmented text generation compared to generation-based models?,"Retrieval-augmented text generation has several advantages compared to generation-based models:

1. Scalability: In retrieval-augmented text generation, knowledge is explicitly acquired from retrieval sources rather than being implicitly stored in model parameters. This allows for greater scalability as the model can easily incorporate new knowledge by retrieving relevant information.

2. Improved Text Generation: Retrieval-augmented models generate text based on retrieved human-written references, which can alleviate the difficulty of text generation. By leveraging existing high-quality text, retrieval-augmented models can produce more informative and fluent responses.

3. Handling Diversity: Generation-based models often struggle with the one-to-many problem, where a single dialogue history can have multiple possible responses. Retrieval-augmented models can address this challenge by retrieving a set of potential responses and selecting the most appropriate one based on retrieval metrics.

Overall, retrieval-augmented text generation combines the strengths of retrieval-based models (informative responses) and generation-based models (generalization capacity) to achieve state-of-the-art performance in many natural language processing tasks.","['A Survey on Retrieval-Augmented Text Generation\nHuayang Li♥,∗\nYixuan Su♠,∗\nDeng Cai♦,∗\nYan Wang♣,∗\nLemao Liu♣,∗\n♥Nara Institute of Science and Technology\n♠University of Cambridge\n♦The Chinese University of Hong Kong\n♣Tencent AI Lab\nli.huayang.lh6@is.naist.jp, ys484@cam.ac.uk\nthisisjcykcd@gmail.com, brandenwang@tencent.com\nlemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented\ngeneration, and then it reviews notable ap-\nproaches according to different tasks including\ndialogue response generation, machine trans-\nlation, and other generation tasks. Finally, it\npoints out some promising directions on top of\nrecent methods to facilitate future research.\n1\nIntroduction\nRetrieval-augmented text generation, as a new\ntext generation paradigm that fuses emerging deep\nlearning technology and traditional retrieval tech-\nnology, has achieved state-of-the-art (SOTA) per-\nformance in many NLP tasks and attracted the at-\ntention of the computational linguistics community\n(Weston et al., 2018; Dinan et al., 2018; Cai et al.,\n2021). Compared with generation-based counter-\npart, this new paradigm has some remarkable ad-\nvantages: 1) The knowledge is not necessary to be\nimplicitly stored in model parameters, but is explic-\nitly acquired in a plug-and-play manner, leading\nto great scalibility; 2) Instead of generating from\nscratch, the paradigm generating text from some re-\ntrieved human-written reference, which potentially\nalleviates the difﬁculty of text generation.\nThis paper aims to review many representative\napproaches for retrieval-augmented text generation\ntasks including dialogue response generation (We-\nston et al., 2018), machine translation (Gu et al.,\n2018) and others (Hashimoto et al., 2018). We\n∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has\nbeen shown beneﬁcial to informative response gen-\neration (Weston et al., 2018; Wu et al., 2019; Cai\net al., 2019a,b). In addition, there has been growing\ninterest in knowledge-grounded generation explor-\ning different forms of knowledge such as knowl-\nedge bases and external documents (Dinan et al.,\n2018; Zhou et al., 2018; Lian et al., 2019; Li et al.,\n2019; Qin et al., 2019; Wu et al., 2021; Zhang et al.,\n2021). On the machine translation task, we summa-\nrize the early work on how the retrieved sentences\n(called translation memory) are used to improve\nstatistical machine translation (SMT) (Koehn et al.,\n2003) models (Simard and Isabelle, 2009; Koehn\nand Senellart, 2010) and in particular, we inten-\nsively highlight several popular methods to inte-\ngrating translation memory to NMT models (Gu\net al., 2018; Zhang et al., 2018; Xu et al., 2020;\nHe et al., 2021). We also review the applications\nof retrieval-augmented generation in other genera-\ntion tasks such as abstractive summarization (Peng\net al., 2019), code generation (Hashimoto et al.,\n2018), paraphrase (Kazemnejad et al., 2020; Su\net al., 2021b), and knowledge-intensive generation\n(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\nRetrieval-Augmented Paradigm'
 '3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\nof possible responses to a single dialogue history\n(i.e., the one-to-many problem). The dialogue his-\ntory alone cannot decide a meaningful and speciﬁc\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\non recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\ntions and possibly post-edited by a human. How-\never, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the other hand,\nthe generation-based models (Shang et al., 2015;\nVinyals and Le, 2015; Li et al., 2016a) generate\na new utterance from scratch. Those generation-\nbased models have better generalization capacity\nwhen handling unseen dialogue contexts. Never-\ntheless, the generated utterances are inclined to be\ndull and non-informative (e.g., “I don’t know”, “I\nthink so”, “Me too” etc.) (Li et al., 2016a).\nShallow Integration\nAs discussed, retrieval-\nbased models may give informative but inappro-\npriate responses while generation-based models\noften do the opposite. It is desirable to combine the\nbest of both worlds. Early work (Qiu et al., 2017)\nattempts to re-rank the output from both models.\nFor a deep integration, Song et al. (2016) and Yang\net al. (2019) extend the standard SEQ2SEQ encoder-\ndecoder model (Bahdanau et al., 2014) with an ex-\ntra encoder for encoding the retrieval result. The\noutput of the extra encoder, along with the output\nfrom the original encoder for dialogue history, is\nused to feed the decoder. Weston et al. (2018) use\na single encoder that takes the concatenation of\nthe original dialogue history and the retrieved as\ninput. Wu et al. (2019) note that the retrieved infor-\nmation should be used in awareness of the context\ndifference, and further proposed to construct an\nedit vector by explicitly encoding the lexical differ-\nences between the input dialogue history and the\nretrieved dialogue history. Pandey et al. (2018) fur-\nther propose to weight different training instances\nby context similarity.\nDeep Integration\nTo prevent the inﬂow of er-\nroneous information, Cai et al. (2019a) propose\na general framework that ﬁrst extracts a skeleton\nfrom the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic structure of an exem-\nplar response, instead of the tokens of the exem-\nplar response, to guide generation. Despite their\ndifferences, a common issue is that the genera-\ntion model easily learns to ignore the retrieved re-\nsponse entirely and collapses to a vanilla seq2seq\nmodel. This happens with improper training in-\nstances. Due to the one-to-many nature, it hap-\npens frequently that a retrieved response (extracted\nskeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,'
 'lemaoliu@gmail.com\nAbstract\nRecently, retrieval-augmented text generation\nattracted increasing attention of the compu-\ntational linguistics community.\nCompared\nwith conventional generation models, retrieval-\naugmented text generation has remarkable ad-\nvantages and particularly has achieved state-of-\nthe-art performance in many NLP tasks. This\npaper aims to conduct a survey about retrieval-\naugmented text generation. It ﬁrstly highlights\nthe generic paradigm of retrieval-augmented']","['Retrieval-augmented text generation has several advantages over conventional generation models. Firstly, the knowledge is not necessarily stored implicitly in model parameters, but is explicitly acquired in a plug-and-play manner, which leads to great scalability. Secondly, instead of generating from scratch, this paradigm generates text from some retrieved human-written reference, which potentially alleviates the difficulty of text generation. This approach has achieved state-of-the-art performance in many NLP tasks.']"
What are the three major components of the retrieval-augmented generation paradigm?,"The three major components of the retrieval-augmented generation paradigm are:

1. Retrieval sources: These are the sources from which relevant instances are retrieved. They can include the training corpus, external datasets, and unsupervised data.

2. Retrieval metrics: These are the metrics used to evaluate the relevance between texts. They can be categorized into sparse-vector retrieval, dense-vector retrieval, and training-based retrieval.

3. Generation models: These are the models used for text generation. They are integrated with the retrieval memory to incorporate the retrieved information into the generation process.","['∗All authors contributed equally.\nﬁrstly present the generic paradigm of retrieval-\naugmented generation as well as three key com-\nponents under this paradigm, which are retrieval\nsources, retrieval metrics and generation models.\nThen, we introduce notable methods about\nretrieval-augmented generation, which are orga-\nnized with respect to different tasks. Speciﬁcally,\non the dialogue response generation task, exem-\nplar/template retrieval as an intermediate step has'
 '(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\nRetrieval-Augmented Paradigm\nIn this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\ny = f(x, z)\n(1)\nwhere z = {⟨xr, yr⟩} is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that yr\nmay beneﬁt the response generation, if xr (or yr)\nis similar (or relevant) to the input x. It is worth\nnoting that xr = ∅ when unsupervised retrieval\nsources are used. In general, the retrieval mem-\nory can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (§2.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n§2.3 we divided them into three categories: sparse-\nvector retrieval, dense-vector retrieval, and training-\nbased retrieval. Finally, how to integrate the re-\ntrieval memory to the generation model is also sig-\nniﬁcant, we also introduce some popular integra-\ntion approaches in §2.4.\n2.2\nRetrieval Sources\nTraining Corpus\nMost previous studies search\nthe external memory from its training corpus (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high\nrelevant scores could be regarded as extra refer-\nences and reduce model’s uncertainty in generation.\nThe main motivation of those works is to to store\nknowledge not only in the model parameters but\nalso in an explicit and accessible form, making the\nmodel be able to re-access it during inference.\nExternal Data\nSome researchers also propose to\nretrieval relevant samples from external datasets\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\nies, the retrieval pool is different with the training\ncorpus, which can further provide additional infor-\nmation that are not contained in the training corpus.\nThis is especially beneﬁcial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main'
 'skeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,\n2019; Cai et al., 2019a) alleviate the above prob-\nlems by putting hard constraints on the data (e.g.,\ndiscarding data with low similarity of the retrieved\nresponse and the target response), which, however,\ngreatly reduces the amount of usable data. Cai\net al. (2019b) employ a random mechanism for\ngenerating the skeletons used for training, which\nextract skeletons from the corresponding responses\nwith some deliberate disturbance. Paranjape et al.\n(2021) propose to model the retriever after the pos-\nterior distribution of retrieval given the input and\nthe target output and train it jointly with the stan-\ndard retriever and the generator by maximizing the\nevidence lower bound (ELBo) in expectation over\nretrieval.\nKnowledge-Enhanced Generation\nThe afore-\nmentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating ˆy from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and efﬁciency for human translators (Dillon\nand Fraser, 2006). As the development of ma-\nchine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statisti-\ncal machine translation (SMT) and neural machine\ntranslation (NMT).\n4.1\nTranslation Memory in SMT\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\nConstrained Decoding with TM\nConstrained\ndecoding is the most straightforward way to in-\ntegrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Gen-\nabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in yr while trans-\nlate other segments by SMT. Speciﬁcally, the ap-\nproach consists of three steps: 1) identify the un-\nmatched segments in both xr and x through the\nedit-distance algorithm; 2) identify the unmatched']","['The three major components of the retrieval-augmented generation paradigm are the retrieval source, retrieval metric, and integration methods.']"
What is the main idea behind the cross-lingual retriever in the context of unsupervised corpus?,"The main idea behind the cross-lingual retriever in the context of unsupervised corpus is to align source-side sentences and their corresponding target-side translations in a dense vector space. This alignment allows the retriever to directly connect the source-side input with the target-side translations, enabling the use of monolingual data in the target language as memories for machine translation.","['pairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-'
 '(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\nRetrieval-Augmented Paradigm\nIn this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\ny = f(x, z)\n(1)\nwhere z = {⟨xr, yr⟩} is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that yr\nmay beneﬁt the response generation, if xr (or yr)\nis similar (or relevant) to the input x. It is worth\nnoting that xr = ∅ when unsupervised retrieval\nsources are used. In general, the retrieval mem-\nory can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (§2.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n§2.3 we divided them into three categories: sparse-\nvector retrieval, dense-vector retrieval, and training-\nbased retrieval. Finally, how to integrate the re-\ntrieval memory to the generation model is also sig-\nniﬁcant, we also introduce some popular integra-\ntion approaches in §2.4.\n2.2\nRetrieval Sources\nTraining Corpus\nMost previous studies search\nthe external memory from its training corpus (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high\nrelevant scores could be regarded as extra refer-\nences and reduce model’s uncertainty in generation.\nThe main motivation of those works is to to store\nknowledge not only in the model parameters but\nalso in an explicit and accessible form, making the\nmodel be able to re-access it during inference.\nExternal Data\nSome researchers also propose to\nretrieval relevant samples from external datasets\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\nies, the retrieval pool is different with the training\ncorpus, which can further provide additional infor-\nmation that are not contained in the training corpus.\nThis is especially beneﬁcial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main'
 'pose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-\nlations, enabling monolingual data in the target\nlanguage to be used alone as memories.\n2.3\nRetrieval Metrics\nSparse-vector Retrieval\nGiven an input se-\nquence x and a retrieval corpus, retrieval model\naims to retrieve a set of relevant examples z =\n{⟨xr, yr⟩} from the corpus. When a supervised\ncorpus is used, {⟨xr, yr⟩} is retrieved by measur-\ning the similarity between x and xr. For simi-\nlarity measurement, sparse-vector retrieval meth-\nods such as TF-IDF and BM25 (Robertson and\nZaragoza, 2009) are widely used. They match key-\nwords efﬁciently with an inverted index.\nDense-vector Retrieval\nHowever, these meth-\nods prefer examples with similar surfaces, and may\nfail to retrieve examples that are only semantically\nrelevant. To alleviate above problem, some stud-\nies (Cao and Xiong, 2018) attempt to retrieve in\ndense-vector space instead of the lexical overlap.\nRecent work (Lee et al., 2019) makes use of pre-\ntrained language models, which encodes the text to\nlow-dimensional dense vectors via BERT-based en-\ncoders. The retrieval score are computed via inner\nproducts between vectors.\nTask-speciﬁc\nRetrieval\nSimilarity-based\nre-\ntrieval is based on a simple heuristic. That is, the\nmore xr resembles with x, the more likely xr\nand yr will help the generation. However, the\nmost similar one by universal textual similarity\ndoes not necessarily serve the best for downstream\nmodels.\nIdeally, the retrieval metric would be\nlearned from the data in a task-dependent way: we\nwish to consider a memory only if it can indeed\nboost the quality of ﬁnal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speciﬁc objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {⟨xr, yr⟩} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efﬁciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various\narchitectures) to encode retrieved target sentences,\nand integrate them through attention (Cao and\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of NLP models, integrating\nretrieved memory through attention becomes a\nvery nature and efﬁcient way.\nSkeleton Extraction\nIn the previous two meth-\nods, the downstream generation model learns how\nto ﬁlter out irrelevant or even harmful informa-\ntion from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., skeleton extraction, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only in-\ntegrate this skeleton in the generation process.\n3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue']","['The main idea behind the cross-lingual retriever in the context of an unsupervised corpus is to align source-side sentences and their corresponding target-side translations in a dense vector space. This is done even when the source-side reference (xr) is absent. As a result, the retriever directly connects the source-side input and target-side translations, enabling monolingual data in the target language to be used alone as memories.']"
What are the two categories of dialogue systems mentioned in the context?,The two categories of dialogue systems mentioned in the context are chit-chat systems and task-oriented systems.,"['3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity'
 '3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\nof possible responses to a single dialogue history\n(i.e., the one-to-many problem). The dialogue his-\ntory alone cannot decide a meaningful and speciﬁc\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\non recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\ntions and possibly post-edited by a human. How-\never, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the other hand,\nthe generation-based models (Shang et al., 2015;\nVinyals and Le, 2015; Li et al., 2016a) generate\na new utterance from scratch. Those generation-\nbased models have better generalization capacity\nwhen handling unseen dialogue contexts. Never-\ntheless, the generated utterances are inclined to be\ndull and non-informative (e.g., “I don’t know”, “I\nthink so”, “Me too” etc.) (Li et al., 2016a).\nShallow Integration\nAs discussed, retrieval-\nbased models may give informative but inappro-\npriate responses while generation-based models\noften do the opposite. It is desirable to combine the\nbest of both worlds. Early work (Qiu et al., 2017)\nattempts to re-rank the output from both models.\nFor a deep integration, Song et al. (2016) and Yang\net al. (2019) extend the standard SEQ2SEQ encoder-\ndecoder model (Bahdanau et al., 2014) with an ex-\ntra encoder for encoding the retrieval result. The\noutput of the extra encoder, along with the output\nfrom the original encoder for dialogue history, is\nused to feed the decoder. Weston et al. (2018) use\na single encoder that takes the concatenation of\nthe original dialogue history and the retrieved as\ninput. Wu et al. (2019) note that the retrieved infor-\nmation should be used in awareness of the context\ndifference, and further proposed to construct an\nedit vector by explicitly encoding the lexical differ-\nences between the input dialogue history and the\nretrieved dialogue history. Pandey et al. (2018) fur-\nther propose to weight different training instances\nby context similarity.\nDeep Integration\nTo prevent the inﬂow of er-\nroneous information, Cai et al. (2019a) propose\na general framework that ﬁrst extracts a skeleton\nfrom the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic structure of an exem-\nplar response, instead of the tokens of the exem-\nplar response, to guide generation. Despite their\ndifferences, a common issue is that the genera-\ntion model easily learns to ignore the retrieved re-\nsponse entirely and collapses to a vanilla seq2seq\nmodel. This happens with improper training in-\nstances. Due to the one-to-many nature, it hap-\npens frequently that a retrieved response (extracted\nskeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,'
 'skeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,\n2019; Cai et al., 2019a) alleviate the above prob-\nlems by putting hard constraints on the data (e.g.,\ndiscarding data with low similarity of the retrieved\nresponse and the target response), which, however,\ngreatly reduces the amount of usable data. Cai\net al. (2019b) employ a random mechanism for\ngenerating the skeletons used for training, which\nextract skeletons from the corresponding responses\nwith some deliberate disturbance. Paranjape et al.\n(2021) propose to model the retriever after the pos-\nterior distribution of retrieval given the input and\nthe target output and train it jointly with the stan-\ndard retriever and the generator by maximizing the\nevidence lower bound (ELBo) in expectation over\nretrieval.\nKnowledge-Enhanced Generation\nThe afore-\nmentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating ˆy from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and efﬁciency for human translators (Dillon\nand Fraser, 2006). As the development of ma-\nchine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statisti-\ncal machine translation (SMT) and neural machine\ntranslation (NMT).\n4.1\nTranslation Memory in SMT\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\nConstrained Decoding with TM\nConstrained\ndecoding is the most straightforward way to in-\ntegrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Gen-\nabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in yr while trans-\nlate other segments by SMT. Speciﬁcally, the ap-\nproach consists of three steps: 1) identify the un-\nmatched segments in both xr and x through the\nedit-distance algorithm; 2) identify the unmatched']",['The two categories of dialogue systems mentioned in the context are chit-chat systems and task-oriented systems.']
What are the three major limitations in existing work for dialogue response generation?,"The three major limitations in existing work for dialogue response generation are:

1. Current methods only use one retrieved response for generation. It can be more beneficial to combine multiple retrieval responses, but this can be difficult due to the one-to-many nature of dialogue response generation.

2. Current methods use a universal relevance score for retrieval. It can be more effective if we can use more customized retrieval metrics, especially for controlled dialogue response generation (e.g., persona, emotion, etc).

3. The retrieval pool of existing methods is limited to dialogue corpora (context-response pairs) or documents. It might be useful to enlarge the retrieval pool by including more corpora in other domains or in other modalities.","['actively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can'
 'skeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,\n2019; Cai et al., 2019a) alleviate the above prob-\nlems by putting hard constraints on the data (e.g.,\ndiscarding data with low similarity of the retrieved\nresponse and the target response), which, however,\ngreatly reduces the amount of usable data. Cai\net al. (2019b) employ a random mechanism for\ngenerating the skeletons used for training, which\nextract skeletons from the corresponding responses\nwith some deliberate disturbance. Paranjape et al.\n(2021) propose to model the retriever after the pos-\nterior distribution of retrieval given the input and\nthe target output and train it jointly with the stan-\ndard retriever and the generator by maximizing the\nevidence lower bound (ELBo) in expectation over\nretrieval.\nKnowledge-Enhanced Generation\nThe afore-\nmentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating ˆy from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and efﬁciency for human translators (Dillon\nand Fraser, 2006). As the development of ma-\nchine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statisti-\ncal machine translation (SMT) and neural machine\ntranslation (NMT).\n4.1\nTranslation Memory in SMT\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\nConstrained Decoding with TM\nConstrained\ndecoding is the most straightforward way to in-\ntegrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Gen-\nabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in yr while trans-\nlate other segments by SMT. Speciﬁcally, the ap-\nproach consists of three steps: 1) identify the un-\nmatched segments in both xr and x through the\nedit-distance algorithm; 2) identify the unmatched'
 '(Lewis et al., 2020b). Finally, we also point out\nsome promising directions on retrieval-augmented\ngeneration to push forward the future research.\n2\nRetrieval-Augmented Paradigm\nIn this section, we ﬁrst give a general formulation\nof retrieval-augmented text generation. Then, we\ndiscuss three major components of the retrieval-\naugmented generation paradigm, including the re-\narXiv:2202.01110v2  [cs.CL]  13 Feb 2022\nInput\nSources \n(Sec. 2.2):\nTraining \nCorpus\nExternal Data\nUnsupervised \nData\nMetrics\n(Sec. 2.3):\nSparse-vector \nRetrieval\nDense-vector \nRetrieval\nTask-specific \nRetrieval\nRetrieval Memory\nGeneration Model\nSec. 4: Machine \nTranslation\nSec. 5: Other \nTasks\nData \nAugmentation\nAttention \nMechanism\nSkeleton & \nTemplates\nInformation Retrieval\nTasks:\nSec. 3: Dialogue \nGeneration\nModels \n(Sec 2.4):\nOutput\nFigure 1: The overview of this survey.\ntrieval source, retrieval metric and integration meth-\nods.\n2.1\nFormulation\nMost text generation tasks can be formulated as a\nmapping from input sequence x to output sequence\ny : y = f(x). For instance, x and y could be the\ndialogue history and the corresponding response\nfor dialogue response generation, the text in the\nsource language and the translation in the target\nlanguage for machine translation, and so on.\nRecently, some researchers suggest to endow\nmodels the capability to access external memory\nvia some information retrieval techniques, so that\nthey can acquire more information in the generation\nprocess (Gu et al., 2018; Weston et al., 2018; Cai\net al., 2019b). The retrieval-augmented generation\ncan be further formulated as:\ny = f(x, z)\n(1)\nwhere z = {⟨xr, yr⟩} is a set of relevant instances\nretrieved from the original training set or external\ndatasets. The main idea of this paradigm is that yr\nmay beneﬁt the response generation, if xr (or yr)\nis similar (or relevant) to the input x. It is worth\nnoting that xr = ∅ when unsupervised retrieval\nsources are used. In general, the retrieval mem-\nory can be retrieved from three kinds of sources:\nthe training corpus, external datasets in the same\nformat with the training corpus, and large-scale\nunsupervised corpus (§2.2). Metrics that evaluate\nthe relevance between text are varied as well, in\n§2.3 we divided them into three categories: sparse-\nvector retrieval, dense-vector retrieval, and training-\nbased retrieval. Finally, how to integrate the re-\ntrieval memory to the generation model is also sig-\nniﬁcant, we also introduce some popular integra-\ntion approaches in §2.4.\n2.2\nRetrieval Sources\nTraining Corpus\nMost previous studies search\nthe external memory from its training corpus (Song\net al., 2016; Gu et al., 2018; Weston et al., 2018).\nIn the inference time, retrieved examples with high\nrelevant scores could be regarded as extra refer-\nences and reduce model’s uncertainty in generation.\nThe main motivation of those works is to to store\nknowledge not only in the model parameters but\nalso in an explicit and accessible form, making the\nmodel be able to re-access it during inference.\nExternal Data\nSome researchers also propose to\nretrieval relevant samples from external datasets\n(Su et al., 2021c; Xiao et al., 2021). In these stud-\nies, the retrieval pool is different with the training\ncorpus, which can further provide additional infor-\nmation that are not contained in the training corpus.\nThis is especially beneﬁcial for applications such\nas domain adaptation and knowledge update. For\nexample, Khandelwal et al. (2020a); Zheng et al.\n(2021a) employ the in-domain dataset as the exter-\nnal memory to achieve fast domain adaptation for\nmachine translation.\nUnsupervised Data\nOne limitation for previous\ntwo sources is that the datasets have to be super-\nvised datasets consisting of aligned input-output\npairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main']","['The three major limitations in existing work for dialogue response generation are: 1) Current methods only use one retrieved response for generation, when it could be more beneficial to combine multiple retrieval responses. However, this can be difficult due to the one-to-many nature of dialogue response generation. 2) Current methods use a universal relevance score for retrieval. It could be more effective if a more customized retrieval metric was used, especially for controlled dialogue response generation (e.g., persona, emotion, etc). 3) The retrieval pool of existing methods is limited to dialogue corpora (context-response pairs) or documents. It might be useful to enlarge the retrieval pool by including more corpora in other domains or in other modalities. These limitations suggest several possible directions for future exploration.']"
What are the limitations of the three kinds of methods discussed in the context?,"The limitations of the three kinds of methods discussed in the context are as follows:

1. Fuzzy score for retrieval: These methods employ fuzzy score for retrieval, which is highly dependent on word matching. As a result, they cannot recall examples that are similar in word semantics but different in surface form.

2. Integration of retrieved examples: The integration methods used in these approaches do not make full use of the knowledge in the retrieved examples. The ways in which the examples are integrated into the SMT module are heuristic and not optimized towards translation quality. The parameter tuning method fine-tunes only a few parameters, which may not be enough to preserve sufficient knowledge from the retrieved examples.

3. Intractability of joint optimization: Since SMT performs in a pipeline manner, it is intractable to jointly optimize retrieval metrics as well as SMT models. As a result, these methods adopt an off-the-shelf metric for retrieval, which can lead to suboptimal performance.","['SMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the ﬁrst two'
 'late other segments by SMT. Speciﬁcally, the ap-\nproach consists of three steps: 1) identify the un-\nmatched segments in both xr and x through the\nedit-distance algorithm; 2) identify the unmatched\nsegments in yr, each of which is aligned to one\nunmatched segment in xr by a word alignment\nalgorithm; 3) decode each unmatched segment in\nx by SMT and then use the result to replace its\ncorresponding unmatched segment in yr. Li et al.\n(2016b) further extend this approach from sentence\nlevel to phrase level. The advantage in constrained\ndecoding is that it does not require to change the\ntranslation model (including phrase table and pa-\nrameters) and can be applied in a plug-and-play\nway. This approach is successful when x is highly\nsimilar to xr; otherwise its performance is de-\ngraded largely, because it explicitly isolates TM\nmatching and SMT decoding and reuses the results\nin xr or not in a deterministic way.\nPhrase Table Aggregation with TM\nThere are\nalso notable efforts to augment the phrase table\nfor SMT by extracting translation rules from the\nretrieved bilingual sentences {⟨xr, yr⟩}.\nThen\nthey re-tune the parameters for the SMT model\nwhich makes use of translation knowledge from\n{⟨xr, yr⟩} in a implicit way when translating x.\nFor example, Biçici and Dymetman (2008); Simard\nand Isabelle (2009) directly combine the extracted\ntranslation rules into the phrase table in a shallow\ncombination way. They introduce an additional fea-\nture to indicate that whether translation rule is from\n{⟨xr, yr⟩} or not and then train all feature weights\nwith MERT (Och, 2003). One characteristic of\nthese work is that a translation rule extracted from\n{⟨xr, yr⟩} which can not exactly match any seg-\nments in x is useless even if it may contain some\nuseful words in its target side. To remedy this ob-\nservation, Wang et al. (2013, 2014) resort to a deep\ncombination way to using the extracted translation\nrules. For each rule in the phrase table, it designs\na generative model to reward the rules which are\nsimilar to those extracted from {⟨xr, yr⟩}. Then\nthis generative model is used as a feature in the log-\nlinear based SMT model whose weight is tuned\ntogether with other features by MERT. In addition,\nLi et al. (2014) employ a similar way to reward\nthe rules but it relies on a discriminative model\nwhich is easy to integrate potential features from\n{⟨xr, yr⟩}.\nParameter Tuning with TM\nUnlike the above\ntwo research lines, Liu et al. (2012, 2014) make use\nof translation memory only in tuning parameters.\nTo be speciﬁc, when translating an input sentence\nx, they ﬁrstly retrieve many similar bilingual sen-\ntences {⟨xr, yr⟩}, and then tune the parameters on\ntop of the retrieved sentences as well as a given de-\nvelopment dataset in a sentence-wise manner, i.e.,\nit performs an independent tuning for each input\nsentence. To improve the efﬁciency of each tuning\nstep, it propose a local update on top of {⟨xr, yr⟩}\nfrom a baseline model.\nDespite the successes of translation memory in\nSMT, there are still some limitations for the above\nthree kinds of methods. Firstly, all these methods\nemploy fuzzy score for retrieval which is highly de-\npendent on word matching and thus can not recall\nsuch examples which are similar in word seman-\ntics but different in surface form. Secondly, these\nmethods integrate the retrieved examples into a\nmodule of SMT in the ways which can not make\nfull use of the knowledge in retrieved examples.\nFor example, the integration ways in the ﬁrst two\nkinds (constrained decoding and phrase table ag-\ngregation) are heuristic and not optimized towards\ntranslation quality; the parameter tuning method\nﬁne-tunes few parameters for log-linear based SMT\nwhich are not enough to preserve sufﬁcient knowl-\nedge from retrieved examples. Thirdly, since SMT\nperforms in a pipeline manner, it is intractable to\njointly optimize retrieval metrics as well as SMT\nmodels. Consequently, all these methods adopt an\noff-the-shelf metric for retrieval, leading to sub-'
 'skeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,\n2019; Cai et al., 2019a) alleviate the above prob-\nlems by putting hard constraints on the data (e.g.,\ndiscarding data with low similarity of the retrieved\nresponse and the target response), which, however,\ngreatly reduces the amount of usable data. Cai\net al. (2019b) employ a random mechanism for\ngenerating the skeletons used for training, which\nextract skeletons from the corresponding responses\nwith some deliberate disturbance. Paranjape et al.\n(2021) propose to model the retriever after the pos-\nterior distribution of retrieval given the input and\nthe target output and train it jointly with the stan-\ndard retriever and the generator by maximizing the\nevidence lower bound (ELBo) in expectation over\nretrieval.\nKnowledge-Enhanced Generation\nThe afore-\nmentioned work demonstrates that retrieval-based\ndialogue systems can be used for building bet-\nter generation-based models. In general, this is\ndone by conditioning the generation on some re-\ntrieved responses. More traditionally, to infuse\nthe response with external knowledge, the retrieval\npool is not necessarily a dialogue corpus. In fact,\nknowledge-grounded dialogue response generation\nexploring different forms of knowledge such as\nknowledge bases and external documents (Dinan\net al., 2018; Zhou et al., 2018; Lian et al., 2019;\nLi et al., 2019; Qin et al., 2019; Wu et al., 2021;\nZhang et al., 2021; Komeili et al., 2021) has been\nactively explored.\nLimitations\nWe note that there are three major\nlimitations in existing work for dialogue response\ngeneration. First, current methods only use one\nretrieved response for generation. It can be more\nbeneﬁcial to combine multiple retrieval responses.\nHowever, this can be difﬁcult due to the one-to-\nmany nature of dialogue response generation. Sec-\nond, current methods use universal relevance score\nfor retrieval. It can be more effective if we can\nuse more customized retrieval metric especially\nfor controlled dialogue response generation (e.g.,\npersona, emotion, etc). Third, the retrieval pool\nof existing methods is limited to dialogue corpora\n(context-response pairs) or documents. It might\nbe useful to enlarge the retrieval pool by including\nmore corpora in other domains or in other modali-\nties. As discussed, there leaves plenty of possible\ndirections to explore in the future.\n4\nMachine Translation\nRetrieval augmented translation originates from hu-\nman translation scenarios (Somers, 2003). When\ntranslating ˆy from an input source sentence x, a hu-\nman translator typically involves a search engine to\nretrieve similar sentences {⟨xr, yr⟩} from a bilin-\ngual database. Such a technique called translation\nmemory is helpful to improve the translation qual-\nity and efﬁciency for human translators (Dillon\nand Fraser, 2006). As the development of ma-\nchine translation techniques, there is a surge of\ninterests in improving machine translation models\nwith translation memory. In the rest of this section,\nwe will review translation memory for both statisti-\ncal machine translation (SMT) and neural machine\ntranslation (NMT).\n4.1\nTranslation Memory in SMT\nGenerally, SMT includes three key components in\na pipeline manner such as phrase table extraction,\nparameter tuning and decoding (Koehn et al., 2003;\nChiang, 2007). As a result, many efforts have been\nmade to make use of translation memory (TM) on\ntop of each component.\nConstrained Decoding with TM\nConstrained\ndecoding is the most straightforward way to in-\ntegrating TM into SMT (Smith and Clark, 2009;\nKoehn and Senellart, 2010; Zhechev and Van Gen-\nabith, 2010; Ma et al., 2011). Its basic idea is\nto reuse the useful segments in yr while trans-\nlate other segments by SMT. Speciﬁcally, the ap-\nproach consists of three steps: 1) identify the un-\nmatched segments in both xr and x through the\nedit-distance algorithm; 2) identify the unmatched']","['The limitations of the three methods discussed in the context are: 1) They all employ a fuzzy score for retrieval which is highly dependent on word matching and thus cannot recall examples that are similar in word semantics but different in surface form. 2) These methods integrate the retrieved examples into a module of SMT in ways that cannot make full use of the knowledge in retrieved examples. For instance, the integration methods in the first two kinds (constrained decoding and phrase table aggregation) are heuristic and not optimized towards translation quality; the parameter tuning method fine-tunes few parameters for log-linear based SMT which are not enough to preserve sufficient knowledge from retrieved examples. 3) Since SMT performs in a pipeline manner, it is intractable to jointly optimize retrieval metrics as well as SMT models. Consequently, all these methods adopt an off-the-shelf metric for retrieval, leading to sub-optimal results.']"
What is the key point of literature in the inference phase of Translation Memory in NMT?,"The key point of literature in the inference phase of Translation Memory in NMT is to reward target words based on words in the retrieved data (yr) during the inference process. This allows for a decision to be made based on both the distribution of the generation model and the additional reward of the retrieval model. Various methods have been proposed to achieve this, such as rewarding target words based on sentence-level similarity between the input sentence (x) and the retrieved sentence (xr), as well as word alignment between xr and yr.","['previous works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to'
 'jointly optimize retrieval metrics as well as SMT\nmodels. Consequently, all these methods adopt an\noff-the-shelf metric for retrieval, leading to sub-\noptimal performance.\n4.2\nTranslation Memory in NMT\nTranslation memory has been widely explored in\nNeural Machine Translation (NMT). Depending\non when retrieval is involved, we can categorize\nprevious works into two classes: 1) an NMT model\nleans how to cooperate with the retrieval model in\nthe training phase; 2) an NMT model is only aware\nof the retrieved data in the inference phase.\nInference Phase\nThe key point of literature in\nthis line is to reward some target words based on\nwords in yr in the inference process. Thus, a de-\ncision can be made based on both the distribution\nof generation model and the additional reward of\nretrieval model. Some previous works propose to\nreward target words based on the sentence-level\nsimilarity between x and xr, and the word align-\nment between xr and yr. Given the input sentence\nx, Zhang et al. (2018) try to assign target words\nin ˆy with higher rewards, when they appear in yr\nand the aligned source words are in both xr and\nx. He et al. (2019) follow a similar framework\nand consider the position information of those tar-\nget words when rewarding. Those works reward\nthe target words in an explicit way, however, the\none-sentence-one-model approach (Li et al., 2016c;\nTurchi et al., 2017) propose to reward target word\nimplicitly. For each testing input x, their approach\nwill ﬁrst ﬁnetune the translation model on retrieved\nmemory {⟨xr, yr⟩} and then translate x.\nOthers try to reward target words based on token-\nlevel similarity score. Most works in this line are\nbased on the dense retriever (Khandelwal et al.,\n2020a), e.g., faiss. Khandelwal et al. (2020a) build\na key-value datastore, where key h(xr, yr\n<t) is the\nhidden state at each time step when translating yr\nfrom xr, and value is its golden-truth target word\nyr\nt. Therefore, in the inference time, they can use\nthe h(x, ˆy<t) as query and reward target words\nwith similar hidden representations in the datas-\ntore. Although this method achieves signiﬁcant\nperformance gain, one drawback of it is the high la-\ntency. To address this issue, Meng et al. (2021) use\nsome heuristics, e.g., pre-ﬁltering, to avoid search-\ning on the entire datastore. The reward score of\nprevious works is got from some non-parametric\napproaches, however, Zheng et al. (2021a) propose\na light-weight network to learn the reward score.\nSince dense retrieval has the potential of cross-\nlingual retrieval, Zheng et al. (2021b) use a similar\napproach to achieve unsupervised domain adapta-\ntion, where a main change is to create the datastore\nbased on synthetic sources sentence and the real\ntarget sentences.\nTraining Phase\nDifferent from those model-\nagnostic approaches, previous works in this line\naim to train the generation model to learn how\nto cooperate with the retrieval model. It is also\nworth noting that most works in this line adopt\nthe sentence-level retrieval, when integrating the\nretrieval information in the training process. To\nachieve its goal, Bulte and Tezcan (2019) and\nHossain et al. (2020) propose a data augmenta-\ntion method to integrate the retrieved information,\nwhere x is concatenated with yr before feeding\ninto the model . Following the data augmentation\napproach, Xu et al. (2020) propose more matching\nmethods to determine including which retrieved\nexample in the source is better.\nThere also exist some works that propose new\narchitectures to integrate the retrieval information.\nUnder the RNN-based framework, Cao and Xiong\n(2018) and Gu et al. (2018) use the gating and at-\ntention mechanism to incorporate the retrieved tar-\nget sentences. When Transformer (Vaswani et al.,\n2017) becomes the backbone of NMT, some works\nalso use additional transformer encoders to en-\ncode retrieved target sentences, and integrate them\nthrough attention mechanism (Bapna and Firat,\n2019; Cao et al., 2019). Xia et al. (2019) repre-'
 'reaches its maximum capacity of M, the earliest\nstored memory is replaced with the new one. This\nprocess is depicted in Figure 2, where we observe\nthat upon reaching the maximum memory capacity,\nthe oldest memory m0 is swapped out and replaced\nby the new memory mt.\n3.3\nRetrieval-Augmented Memory Generator\nUpon receiving a request from the augmentation\ncoordinator, the memory generator on the cloud\ninitiates the preparation of the augmented memory,\nwhich is then returned to the client. The memory\npreparation process consists of two steps: docu-\nment retrieval and memory generation.\n3.3.1\nDocument Retrieval\nGiven an input context x, the goal of the retriever\nis to select the top-k most relevant documents\nPrompt\nGenerate a list of key takeaways in complete sentences for each paragraph one by one, containing all the details such as entities,\npersons, organizations, numbers, years, and locations.\nP1: Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that uses deep\nlearning to produce human-like text. Given an initial text as prompt, it will produce text that continues the prompt.\nP2: The architecture is a decoder-only transformer network with a 2048-token-long context and then-unprecedented size of\n175 billion parameters, requiring 800GB to store. ... This eliminated the need for human supervision and for time-intensive\nhand-labeling.\nKey Takeaways: ### P1:\nOutput\n- Generative Pre-trained Transformer 3 (GPT-3) is a deep learning-based autoregressive language model released in 2020 that\nproduces human-like text.\n- It takes an initial text as prompt and produces text that continues the prompt.\n### P2:\n- GPT-3 has a decoder-only transformer network with a 2048-token-long context and 175 billion parameters, requiring 800GB\nto store.\n- It is trained to predict what the next token is based on previous tokens using generative pre-training. ...\nTable 1: Example of Augmented Memory Generated by GPT3 Model.\nDr = {d1, . . . , dk} from a large retrieval corpus\nD, where Dr ⊆ D. Following prior work (Lewis\net al., 2020; Ni et al., 2021), we use the Dense\nPassage Retrieval (DPR) (Karpukhin et al., 2020)\nmethod, which is based on a dual encoder archi-\ntecture pre-trained for question and answering task.\nDPR encodes the document and the query with a\ncontext encoder and a query encoder respectively,\nand calculates the cosine similarity of the encoded\nembeddings to retrieve the top-k most relevant doc-\numents. We use the FAISS library (Johnson et al.,\n2019) for efficient retrieval.\n3.3.2\nMemory Generation\nAfter retrieving the relevant documents Dr, instead\nof directly using them as in Lewis et al. (2020),\nwe employ a LLM such as GPT3/3.5 to generate\nconcise bullet points that capture the essential in-\nformation from the retrieved documents. These\nbullet points serve as the augmented memory, sig-\nnificantly reducing its size by including only the\nkey takeaways instead of the full documents. This\nreduces both the communication cost to the client\nand the inference cost of the client model.\nTo generate summary bullet points from a re-\ntrieved document d ∈ Dr, we first split the whole\ndocument into paragraphs d = {p1, . . . , pl}, where\nl is the number of paragraphs. We choose an ap-\npropriate paragraph size that maintains sentence\nintegrity, avoiding breaking sentences in the mid-\ndle. Once the paragraphs are created, we utilize the\nGPT3 Davinci model to extract the key takeaways\nfrom each paragraph pi, as outlined in Table 1. To\nminimize the frequency of GPT3 call requests, we\nconsolidate multiple paragraphs within a document.\nTable 1 illustrates an example where bullet points\nare generated for two paragraphs. Subsequently, all\nthe generated bullet points from the retrieval docu-\nments are merged to form the memory mt for the\ncurrent t-th memory request. This memory is then\ncombined with the existing memory to construct\nthe new M by the augmentation coordinator, as\nelucidated in Section 3.2.\n3.4']","['The key point of literature in the inference phase of Translation Memory in NMT is to reward some target words based on words in yr in the inference process. This allows a decision to be made based on both the distribution of the generation model and the additional reward of the retrieval model. Some previous works propose to reward target words based on the sentence-level similarity between x and xr, and the word alignment between xr and yr. Other works reward target words based on token-level similarity score. Some methods also use a light-weight network to learn the reward score.']"
What is the proposed method by Cai et al. (2021) for retrieving the most similar target sentence in a monolingual dataset?,"Cai et al. (2021) propose a cross-lingual retriever that directly retrieves the most similar target sentence from a monolingual dataset. They align source-side sentences and the corresponding target-side translations in a dense vector space, connecting the source-side input and target-side translations. This allows monolingual data in the target language to be used as memories for retrieval.","['pairs. For machine translation, Cai et al. (2021) pro-\npose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-'
 'also use additional transformer encoders to en-\ncode retrieved target sentences, and integrate them\nthrough attention mechanism (Bapna and Firat,\n2019; Cao et al., 2019). Xia et al. (2019) repre-\nsent the retrieved target sentences in a different\ndata structure, i.e., a graph structure, and integrate\nit through attention mechanism. He et al. (2021)\npropose a light-weight method to encode the re-\ntrieved target sentences and leverage the alignment\ninformation to ﬁlter out irrelevant information. Dif-\nferent from previous works that rely on bilingual\nmemories, Cai et al. (2021) propose a framework\nthat can retrieve the most similar target sentence in\na monolingual dataset, using a source sentence as\nquery.\nLimitations\nIn the section of SMT, we have\nshowed some limitations of the retrieval augmented\napproaches. There also exist some limitations in\nthe line of NMT. First, the information used for\nderiving reward scores is limited. The similarity\nbetween an input and retrieved examples is the\nprimary feature to derive reward scores.\nHow-\never, some information, e.g., frequencies of words\nand context, may also be beneﬁcial for integrating\nthe translation memory. Second, it remains to be\nan open question that when should we use the re-\ntrieved information and when not. In the inference\nphase, approaches tend to integrate the translation\nmemory excessively, e.g., at each time step, which\nnot only reduces the translation efﬁciency but may\nalso dampen the ﬂuency of generated results.\n5\nOther Tasks\nIn addition to dialogue system and machine trans-\nlation, retrieval-augmented generation techniques\nhave shown to be beneﬁcial in many other tasks. In\nthe following, we highlight several key tasks that\napply retrieval-augmented generation approaches.1\nLanguage Modelling\nIt has been shown that\nproperly leveraging information from retrieval\nmemory could improve the performance of large\npre-trained language model. To build a more accu-\nrate language model, Khandelwal et al. (2020b) pro-\npose to incorporate a soft memory module into the\nsystem. Speciﬁcally, an index is built by caching\nthe hidden states of the training corpus. Then, the\nlanguage model accesses the index via k-NN search\nand displays a greatly improved performance. As\nanother example, Guu et al. (2020) propose a new\nparadigm that applies retrieval-augmented tech-\nnique into the pre-training of generative language\nmodel. During learning, they train a neural se-\nlector that dynamically samples a relevant text to\nguide the reconstruction of a corrupted input se-\nquence. In this way, the pre-trained model deliv-\ners better results by explicitly grounding on the\nretrieval memory. Lewis et al. (2020a) combine\nlanguage model pre-training with a paraphrasing\n1Here, we focus on tasks other than question answering.\nWe refer readers interested in QA to Chen and Yih (2020).\napproach. During learning, an input sequence to\nthe model is ﬁrst corrupted. In the meantime, a set\nof multi-lingual texts are retrieved based on which\nthe model learns to reconstruct the original input\nsequence. Recently, Borgeaud et al. (2021) pro-\npose RETRO, a large pre-trained language model\nenhanced with retrieved documents, and obtained\ncomparable performances with GPT-3 using 25×\nfewer parameters.\nSummarization\nText summarization is another\nresearch\narea\nthat\nbeneﬁts\nfrom\nretrieval-\naugmented text generation.\nPeng et al. (2019)\npropose an adaptive decoding framework which\nﬁrst retrieves an exemplar document given the\nsource document. Then, the summarization of the\nsource document is derived through an adaptive\ngeneration process based on the retrieved template.\nDifferent from Peng et al. (2019), Cao et al.\n(2018) and Hossain et al. (2020) introduce an\nintermediate re-ranking stage into the generation\npipeline.\nSpeciﬁcally, before generating the\ndocument summary, the retrieval documents are\nﬁrst re-ranked based on their similarity scores\nwith respect to the source document. Then, the'
 'pose a cross-lingual retriever to directly retrieve tar-\nget sentence from unsupervised corpus (i.e., mono-\nlingual corpus in the target language). The main\nidea is aligning source-side sentences and the corre-\nsponding target-side translations in a dense vector\nspace, i.e., aligning x and yr when xr is absent.\nAs a result, the retriever directly connects the dots\nbetween the source-side input and target-side trans-\nlations, enabling monolingual data in the target\nlanguage to be used alone as memories.\n2.3\nRetrieval Metrics\nSparse-vector Retrieval\nGiven an input se-\nquence x and a retrieval corpus, retrieval model\naims to retrieve a set of relevant examples z =\n{⟨xr, yr⟩} from the corpus. When a supervised\ncorpus is used, {⟨xr, yr⟩} is retrieved by measur-\ning the similarity between x and xr. For simi-\nlarity measurement, sparse-vector retrieval meth-\nods such as TF-IDF and BM25 (Robertson and\nZaragoza, 2009) are widely used. They match key-\nwords efﬁciently with an inverted index.\nDense-vector Retrieval\nHowever, these meth-\nods prefer examples with similar surfaces, and may\nfail to retrieve examples that are only semantically\nrelevant. To alleviate above problem, some stud-\nies (Cao and Xiong, 2018) attempt to retrieve in\ndense-vector space instead of the lexical overlap.\nRecent work (Lee et al., 2019) makes use of pre-\ntrained language models, which encodes the text to\nlow-dimensional dense vectors via BERT-based en-\ncoders. The retrieval score are computed via inner\nproducts between vectors.\nTask-speciﬁc\nRetrieval\nSimilarity-based\nre-\ntrieval is based on a simple heuristic. That is, the\nmore xr resembles with x, the more likely xr\nand yr will help the generation. However, the\nmost similar one by universal textual similarity\ndoes not necessarily serve the best for downstream\nmodels.\nIdeally, the retrieval metric would be\nlearned from the data in a task-dependent way: we\nwish to consider a memory only if it can indeed\nboost the quality of ﬁnal generation. To this end,\nCai et al. (2021) propose to unify the memory\nretriever and its downstream generation model\ninto a learnable whole. Such memory retrieval is\nend-to-end optimized for task-speciﬁc objectives.\n2.4\nIntegration\nData Augmentation\nThere are several ways to\nintegrate the retrieved external memory in gener-\nation. One straightforward way is data augmen-\ntation, which constructs some augmented inputs\nby concatenating spans from {⟨xr, yr⟩} with the\noriginal input x. By training on the augmented\ninputs, a generation model implicitly leans how\nto integrate the retrieved information. Despite the\nsimplicity, this kind of methods works efﬁciently\nin lots of tasks (Song et al., 2016; Weston et al.,\n2018; Bulte and Tezcan, 2019).\nAttention\nMechanisms\nAnother\nintegration\nmethod\nis\nbased\non\nattention\nmechanisms\n(Bahdanau et al., 2014). The main idea of this\nfashion is adopting additional encoders (in various\narchitectures) to encode retrieved target sentences,\nand integrate them through attention (Cao and\nXiong, 2018; Gu et al., 2018; Bapna and Firat,\n2019). Since the attention mechanism is becoming\n(Bahdanau et al., 2014; Vaswani et al., 2017) a\nkey module in lots of NLP models, integrating\nretrieved memory through attention becomes a\nvery nature and efﬁcient way.\nSkeleton Extraction\nIn the previous two meth-\nods, the downstream generation model learns how\nto ﬁlter out irrelevant or even harmful informa-\ntion from the retrieved examples implicitly. There\nalso exist some works that try to explicitly extract\nuseful information, i.e., skeleton extraction, from\nthe retrieved memory (Cai et al., 2019a; Wu et al.,\n2019; Cai et al., 2019b). For example, one skeleton\nshould be a part of a whole utterance with irrelevant\ncontent masked, and the generation model only in-\ntegrate this skeleton in the generation process.\n3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue']",['Cai et al. (2021) propose a framework that retrieves the most similar target sentence in a monolingual dataset using a source sentence as a query.']
What is the proposed framework for paraphrase generation by Kazemnejad et al. (2020)?,"The proposed framework for paraphrase generation by Kazemnejad et al. (2020) involves a two-step process. First, a sentence that is similar to the input sentence is retrieved. Then, a neural editor uses the retrieved sentence to produce the paraphrased sentence.","['the selected templates.\nParaphrase Generation\nTo address the lack of\nquality as well as diversity in the generation of para-\nphrases, Kazemnejad et al. (2020) propose a gen-\neration framework which ﬁrst retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in'
 'pipeline.\nSpeciﬁcally, before generating the\ndocument summary, the retrieval documents are\nﬁrst re-ranked based on their similarity scores\nwith respect to the source document. Then, the\ndocument summarization is produced by re-writing\nthe selected templates.\nParaphrase Generation\nTo address the lack of\nquality as well as diversity in the generation of para-\nphrases, Kazemnejad et al. (2020) propose a gen-\neration framework which ﬁrst retrieves a sentence\nthat is similar to input sentence. Then, based on\nthe retrieved sentence, a neural editor produces the\nresulting paraphrased sentence. Chen et al. (2019)\ninvestigate a different aspect of paraphrasing, i.e.\nhow to control the linguistic syntax displayed in\nthe generated text. To achieve this goal, Chen et al.\n(2019) propose to ﬁrst extract a sentential exem-\nplar that serves as the syntax template. A neural\nmodel then generates the paraphrase with desired\nlinguistic syntax following the retrieved exemplar.\nText Style Transfer\nTo improve the quality of\ngenerated text, Li et al. (2018) propose a retrieval-\naugmented framework which ﬁrst retrieves texts\nthat are similar to the input based on lexical-level\nsimilarity. Then, the retrieved tokens that are irrel-\nevant to the source are deleted, and the output is\nderived from the edited template. Xiao et al. (2021)\nalso adopte this framework by incorporating re-\ntrieval information from two sources (i.e. sparse\nand dense memories) and obtained an improved\nmodel performance.\nData-to-Text Generation\nRecently, retrieval-\naugmented generation has been adapted to the task\nof data-to-text generation. To bridge the gap be-\ntween the structured data and natural language\ntext, Su et al. (2021a) propose a novel retrieval-\naugmented framework.\nSpeciﬁcally, given the\nsource data, a set of candidate texts are ﬁrst re-\ntrieved from a large unlabelled corpus. Then, a\nneural selector is applied to measure the similari-\nties between the source data and candidate texts,\nand extract a set of more ﬁne-grained prototypes\nfrom the candidates. Lastly, a generation model\ntakes the prototypes as input to produce the text\nthat describes the given structured data.\nWhile retrieval-augmented generation has been\nwidely explored in the NLP community, we sug-\ngest that future research could extend this approach\nto tasks that involve data from multiple modali-\nties. For instance, with recent advancements in\nimage-text retrieval (Jia et al., 2021; Radford et al.,\n2021), the structural gap between images and texts\nis largely bridged. Some early studies (Zhang et al.,\n2020) have shown that information retrieved from\nimages could improve the performance of neural\nmachine translation model. Naturally, such meth-\nods could be extended to other multi-modal tasks,\nsuch as image captioning (Karpathy and Li, 2015).\nA similar idea could also be applied to tasks be-\nyond images, such as speech-to-text transcription\n(Gales and Young, 2007).\n6\nFuture Directions\nDespite the current success of retrieval augmented\ntext generation, there is still a long way to go as\ndiscussed in previous sections. We highlight some\ndirections to facilitate the future research as fol-\nlows:\nRetrieval Sensitivity\nThe performance of re-\ntrieval augmented text generation is very sensitive\nto the retrieval quality, i.e., the similarity between\nthe query and the retrieved examples. Currently, re-\ntrieval augmented text generation models perform\nwell when the retrieved examples are very simi-\nlar to the query. However, they are even worse\nthan the generation models without retrieval when\nthe retrieval examples are less similar. Therefore,\nit would be important to exploit new methods to\naddress such an issue on similarity.\nRetrieval Efﬁciency\nGenerally, if one enlarges\nthe retrieval memory to some extent, it would be\npossible to retrieve an example which is very simi-\nlar to the query.Unfortunately, the downside is that\nthe overall inference for the retrieval augmented'
 '3\nDialogue Response Generation\nBackground\nDialogue systems can be grouped\ninto two categories: chit-chat systems and task-\noriented systems. While task-oriented dialogue\nsystems are designed to accomplish speciﬁc user\ntasks such as air tickets booking, chit-chat dialogue\nsystems aim at giving a meaningful and ﬂuent re-\nsponse for any dialogue history in the open domain.\nDialogue response generation in chit-chat dialogue\nsystem is challenging partly due to the diversity\nof possible responses to a single dialogue history\n(i.e., the one-to-many problem). The dialogue his-\ntory alone cannot decide a meaningful and speciﬁc\nresponse. Also, external knowledge that is not\npresent in the dialogue history are often necessary\nfor avoiding safe but boring responses. We focus\non recent efforts tackling the challenges to develop\nchit-chat dialogue systems.\nMost modern chit-chat dialogue systems can\nbe categorized into two classes, namely, retrieval-\nbased models and generation-based models. The\nretrieval-based models (Ji et al., 2014; Hu et al.,\n2014) directly copy an existing response from cu-\nrated dialogue corpora (i.e., the retrieval pool)\nwhen receiving a response request. The retrieved\nresponses are often informative and grammatical\nas they are collected from real-world conversa-\ntions and possibly post-edited by a human. How-\never, such systems perform poorly when a given\ndialogue history is substantially different from\nthose in the retrieval pool. On the other hand,\nthe generation-based models (Shang et al., 2015;\nVinyals and Le, 2015; Li et al., 2016a) generate\na new utterance from scratch. Those generation-\nbased models have better generalization capacity\nwhen handling unseen dialogue contexts. Never-\ntheless, the generated utterances are inclined to be\ndull and non-informative (e.g., “I don’t know”, “I\nthink so”, “Me too” etc.) (Li et al., 2016a).\nShallow Integration\nAs discussed, retrieval-\nbased models may give informative but inappro-\npriate responses while generation-based models\noften do the opposite. It is desirable to combine the\nbest of both worlds. Early work (Qiu et al., 2017)\nattempts to re-rank the output from both models.\nFor a deep integration, Song et al. (2016) and Yang\net al. (2019) extend the standard SEQ2SEQ encoder-\ndecoder model (Bahdanau et al., 2014) with an ex-\ntra encoder for encoding the retrieval result. The\noutput of the extra encoder, along with the output\nfrom the original encoder for dialogue history, is\nused to feed the decoder. Weston et al. (2018) use\na single encoder that takes the concatenation of\nthe original dialogue history and the retrieved as\ninput. Wu et al. (2019) note that the retrieved infor-\nmation should be used in awareness of the context\ndifference, and further proposed to construct an\nedit vector by explicitly encoding the lexical differ-\nences between the input dialogue history and the\nretrieved dialogue history. Pandey et al. (2018) fur-\nther propose to weight different training instances\nby context similarity.\nDeep Integration\nTo prevent the inﬂow of er-\nroneous information, Cai et al. (2019a) propose\na general framework that ﬁrst extracts a skeleton\nfrom the retrieved response and then generates the\nresponse based on the extracted skeleton. This\nframework is also adopted for stylistic response\ngeneration (Su et al., 2021c). Gupta et al. (2021)\nsuggest to use the semantic structure of an exem-\nplar response, instead of the tokens of the exem-\nplar response, to guide generation. Despite their\ndifferences, a common issue is that the genera-\ntion model easily learns to ignore the retrieved re-\nsponse entirely and collapses to a vanilla seq2seq\nmodel. This happens with improper training in-\nstances. Due to the one-to-many nature, it hap-\npens frequently that a retrieved response (extracted\nskeleton) is suitable for responding to the query,\nbut inconsistent with the current target response.\nEarlier studies (Weston et al., 2018; Wu et al.,']","['Kazemnejad et al. (2020) proposed a paraphrase generation framework that first retrieves a sentence similar to the input sentence. Then, a neural editor produces the resulting paraphrased sentence based on the retrieved sentence.']"
What is the purpose of this paper?,The purpose of this paper is to address the issue of ambiguous questions and provide a framework for identifying and answering the different interpretations of such questions. It aims to help readers understand how to approach ambiguous questions and provide accurate and relevant answers based on the different possible interpretations.,"['2018. Retrieval of the best counterargument without\nprior topic knowledge. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 241–251,\nMelbourne, Australia. Association for Computational\nLinguistics.\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu\nWang, Madeleine van Zuylen, Arman Cohan, and\nHannaneh Hajishirzi. 2020. Fact or ﬁction: Verifying\nscientiﬁc claims. In Proceedings of the 2020 Con-'
 'interpretations, we need to consider the different charts and regions where Roy Orbison’s music was popular. Roy Orbison is\npopular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this question has 2 interpretations: (1) What\nwas Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What was Roy Orbison’s ﬁrst number one hit in\nthe UK and Ireland? The answers to all interpretations are: (1) Running Scared was the ﬁrst number one hit for Roy Orbison\nin the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was the ﬁrst number one hit for Roy Orbison in\nthe UK and Ireland.\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\nQuestion: What is the criminal’s name in the breakfast club?\nAnswer: This question is ambiguous in terms of which speciﬁc name is being referred to - the character’s name or the actor’s\nname. In order to ﬁgure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\nThe Breakfast Club.\nGiven an ambiguous question, ﬁgure out its interpretations and answer them one by one.\nQuestion: How many state parks are there in virginia?\nAnswer:\nPrompt C.6: exemplars of ASQA-hint\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpretations and\nanswer them one by one.\nQuestion: Who played bonnie in gone with the wind?\nHint: This question is ambiguous in terms of which version or adaptation of Gone with the Wind is being referred to.\nAnswer: In order to ﬁgure out its interpretations, we need to consider different versions or adaptations of Gone with the Wind.\nGone with the Wind has two versions or adaptations: the 1939 ﬁlm Gone with the Wind or the 2008 musical Gone with the\nWind. Therefore, this question has 2 interpretations: (1) Who played Bonnie in the 1939 ﬁlm Gone with the Wind? (2) Who\nplayed Bonnie in the 2008 musical Gone with the Wind? The answers to all interpretations are: (1) The 1939 ﬁlm Gone with\nthe Wind’s character Bonnie was played by Eleanore Cammack ""Cammie"" King. (2) The 2008 musical Gone with the Wind’s\ncharacter Bonnie was played by Leilah de Meza.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\ntions and answer them one by one.\nQuestion: What is the second largest city in the usa?\nHint: This question is ambiguous in terms of the criteria being used to determine the second largest city in the USA.\nAnswer: In order to ﬁgure out its interpretations, we need to consider different criteria to determine a city’s size. City size can\nbe measured by two criteria: population or area. Therefore, this question has 2 interpretations: (1) What is the second largest\ncity in the USA by population? (2) What is the second largest city in the USA by area? The answers to all interpretations are:\n(1) The second largest city in the USA by population is Los Angeles, California. (2) The second largest city in the USA by\narea is Juneau, Alaska.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\ntions and answer them one by one.\nQuestion: When was bohemian rhapsody released as a single?\nHint: This question is ambiguous in terms of which country’s release of the single is being referred to.\nAnswer: In order to ﬁgure out its interpretations, we need to consider different countries where Bohemian Rhapsody is\nreleased. Bohemian Rhapsody was released in the United Kingdom and in the United States on different dates. Therefore,'
 'music was popular. Roy Orbison is popular in both the US Hot 100 and Canada, and the UK and Ireland. Therefore, this\nquestion has 2 interpretations: (1) What was Roy Orbison’s ﬁrst number one hit in the US Hot 100 and Canada? (2) What\nwas Roy Orbison’s ﬁrst number one hit in the UK and Ireland? The answers to all interpretations are: (1) Running Scared\nwas the ﬁrst number one hit for Roy Orbison in the US Hot 100 and Canada. (2) Only the Lonely (Know the Way I Feel) was\nthe ﬁrst number one hit for Roy Orbison in the UK and Ireland.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\ntions and answer them one by one.\nQuestion: What is the criminal’s name in the breakfast club?\nHint: This question is ambiguous in terms of which speciﬁc name is being referred to - the character’s name or the actor’s\nname.\nAnswer: In order to ﬁgure out its interpretations, we need to consider both possibilities: the character’s name or the actor’s\nname. Therefore, this question has 2 interpretations: (1) What is the criminal’s character name in The Breakfast Club? (2)\nWhat is the the name of the actor who played the criminal in The Breakfast Club? The answers to all interpretations are: (1)\nJohn Bender was the name of the criminal’s character in The Breakfast Club. (2) Judd Nelson was the actor of the criminal in\nThe Breakfast Club.\nGiven an ambiguous question and a hint on which aspect of the question is ambiguous, ﬁgure out its interpreta-\ntions and answer them one by one.\nQuestion: How many state parks are there in virginia?\nHint: This question is ambiguous in terms of the time frame or period being referred to.\nAnswer: In order to ﬁgure out its interpretations,\nPrompt C.8: exemplars of WikiAsp\nGenerate a summary about Aslanhane Mosque including the following aspects: location, history with one aspect per line.\n# Location\nThe mosque is in the old quarter of ankara next to ankara castle. With an altitude of 947 metres (3,107 ft) it overlooks ankara\nat 39°56’12""N 32°51’55""E.\n# History\nThe mosque is one of the oldest mosques in Turkey still standing. It was built during the reign of Mesud II of the Anatolian\nSeljuks in 1290. Its architect was Ebubekir Mehmet. It was commissioned by two Ahi leaders named Hüsamettin and\nHasaneddin. However, in 1330, it was repaired by another Ahi leader named ¸Serafettin after whom the mosque was named.\nAfter several minor repairs the mosque was restored by the directorate general of foundations in 2010-2013 term.\nGenerate a summary about Untold Legends:\nThe Warrior’s Code including the following aspects:\nreception,\ngameplay, development with one aspect per line.\n# Reception\nThe game received ""mixed or average reviews"" according to video game review aggregator Metacritic.\n# Gameplay\nThe warrior’s code is a hack n’ slash action role-playing game, which concentrates on action-oriented combat.\n# Development\nAs a pre-order bonus, the game was shipped with a small action ﬁgure of the Guardian class.\nGenerate a summary about Raid on St.\nAugustine including the following aspects: aftermath, background with\none aspect per line.\n# Aftermath\nOnce the English had gone Menéndez and the rest of the Spanish settlers returned to ﬁnd a smoldering ruins and very little\nleft. He soon and begged for help from the viceroy of Cuba and the settlement took a while to build itself back up. The\ndestroyed fort was replaced with the present day Castillo de San Marcos.\n# Background\nWar had already been unofﬁcially declared by Philip II of Spain after the Treaty of Nonsuch in which Elizabeth I had\noffered her support to the rebellious Protestant Dutch rebels. The Queen through Francis Walsingham ordered Sir Francis\nDrake to lead an expedition to attack the Spanish New World in a kind of preemptive strike. Sailing from Plymouth,\nEngland, he struck ﬁrst at Santiago in November 1585 then across the Atlantic at the Spanish new world city of Santo']","['The purpose of this paper is to conduct a survey about retrieval-augmented text generation. It highlights the generic paradigm of retrieval-augmented generation and reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. The paper also points out some important directions on top of recent methods to facilitate future research.']"
